---
layout: post
category: RNN
---

# LSTM : Long Short Term Memory

## Table of contents
- [Purpose](#purpose)
- [Idea](#idea)
- [Activation Functions](#activation-functions)
- [Architecture](#architecture)
- [Example](#example)

## [Purpose](#purpose)

LSTM is advanced algorithm of vanilla Recurrent Neural Network, developed to avoid exploding and vanishing gradient problem of RNN.

## [Idea](#idea)

LSTM uses two seperate paths to make predictions.

- Long-term memory
- Short-term memory

## [Activation Functions](#activation-functions)

- Sigmoid
  : Any x-axis coordinate map into y-axis coordinate between **0 and 1**.
  
  ![Sigmoid activation f](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png)

- tanh
  : Any x-axis coordinate map into y-axis coordinate between **-1 and 1**.
  
  ![tanh activation f](https://vidyasheela.com/web-contents/img/post_img/39/tanh%20activation%20function-new.png)

## [Architecture](#architecture)

- **Long Term Memory**
  - No weights or biases
  - Prevent gradient from exploding or vanishing
- **Short Term Memory**
  - Directly connected to weights
 
### Stages of a unit
#### Stage 1.
- The first stage in a LSTM unit determines what percentage of the Long Term Memory is remembered.
- Also called as "Forget Gate"
#### Stage 2.
- This stage determines how we should update the Long Term Memory.
- Also called as "Input Gate"
#### Stage 3.
- The final stage calculates output from entire LSTM unit.
- Also called as "Output Gate"

## [Example](#exmaple)

